# -*- coding: utf-8 -*-
"""ICD_VideoGameSales.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/12HHVWdrgyZfQy-Y_pXIYw92-qnFUpDw8
"""

import os
import numpy as np
import pandas as pd

import warnings
warnings.filterwarnings("ignore") #ignorar avisos

#ploting
import matplotlib.pyplot as plt
import seaborn as sns
import plotly.graph_objects as go

#ML
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split
from sklearn.metrics import confusion_matrix
from sklearn.metrics import classification_report
from sklearn.metrics import accuracy_score
from sklearn.metrics import r2_score
from sklearn.model_selection import cross_val_score
from sklearn.linear_model import Ridge
from sklearn.tree import DecisionTreeRegressor
from sklearn.linear_model import LinearRegression
from sklearn.ensemble import RandomForestRegressor
from sklearn.neighbors import KNeighborsRegressor

#++++++++++++++ AVALIAÇÃO E ENTENDIMENTO DOS DADOS ++++++++++++++

#Tipos de dados
data = pd.read_csv('/content/vgsales.csv')
data.info()

data.shape

data.columns

data.describe()

data.isnull().sum().sort_values(ascending=False)

registros_com_ano_nulo = data[data['Year'].isnull()].head()
print(registros_com_ano_nulo)

registros_com_Publisher_nulo = data[data['Publisher'].isnull()].head()
print(registros_com_Publisher_nulo)

#TEMOS DOIS VALORES NAN [Year, Publisher] COM BAIXA INCIDÊNCIA
#YEAR = 1,6327%
#PUBLISHER = 0,3494%

#PORCENTAGEM DE VALORES DE ANO AUSENTES POR PLATAFORMA
data2= data.copy()[['Year', 'Platform']] #COPIA DATA PARA DATA2 COM YEAR E PLATAFORM
data2['flag']= data2['Year'].isna().astype(int) #CRIA UMA NOVA COLUNA CHAMADA 'FLAG' EM DATA2, QUE RECEBE 1 SE O VALOR NA COLUNA 'YEAR' NAN
data3= data2.groupby('Platform',as_index=False)['flag'].sum()#AGRUPA 'DATA2' PELA COLUNA 'PLATFORM'. CONTA O NÚMERO DE VALORES NULOS NA COLUNA 'YEAR' PARA CADA PLATAFORMA
data3['% de NAN - Year']= data3.apply(lambda x: x['flag']/data[data['Platform']==x['Platform']].shape[0],axis=1)
data3.sort_values('% de NAN - Year',ascending=False, inplace=True)
print(data3)

data['Publisher'].isna().groupby(data['Year']).sum().plot(title='Ausencia de "Publisher" por ano');

data_organizado = data.sort_values(by='Rank').head()
print(data_organizado)

data.dtypes

cat_cols = []
for col in data:
    if data[col].dtype == 'object':
        cat_cols.append(col)
cat_cols #DF DE COLUNAS DE CATEGORIAS

num_cols = []
for col in data:
    if data[col].dtype != 'object':
        num_cols.append(col)
num_cols #DF DE COLUNAS NÚMERICAS

data[num_cols].describe()

sales_cols= ['NA_Sales','EU_Sales', 'JP_Sales', 'Other_Sales', 'Global_Sales'] #DF DE COLUNAS DE VENDAS

plt.figure(figsize=(20,10))
for i, column in enumerate(sales_cols):
    plt.subplot(3,2,i+1)
    sns.distplot(data[column], bins=20)

#VENDAS DISTORCIDAS A DIREITA, COM QUASE TODA DISTRIBUIÇÃO <0.1

#FUNÇÃO LAMBDA QUE CALCULA A ASSIMETRIA
skew_num= pd.DataFrame({'columns':sales_cols})
skew_num['skewness']= skew_num['columns'].apply(lambda x:data[x].skew())
skew_num

#CALCULO LIMITE
def upper_boundary_skew(data,col,dis):
    Q_1= data[col].quantile(0.25) #PRIMEIRO 25%
    Q_3= data[col].quantile(0.75) #TERCEIRO 75%
    IQR= Q_3 - Q_1
    return Q_3 + (IQR*dis)

extreme_skew = pd.DataFrame({'columns':sales_cols})

extreme_skew['upper_boundary']= extreme_skew['columns'].apply(lambda x: upper_boundary_skew(data,x,1.5))
extreme_skew['obs_upper']= extreme_skew.apply(lambda x:data[data[x['columns']]>x['upper_boundary']].shape[0], axis=1)
extreme_skew['extreme_upper_boundary']= extreme_skew['columns'].apply(lambda x: upper_boundary_skew(data,x,3))
extreme_skew['extreme_obs_upper']= extreme_skew.apply(lambda x:data[data[x['columns']]>x['extreme_upper_boundary']].shape[0], axis=1)
extreme_skew

#REMOVENDO VALORES NULOS, UMA DAS CAUSAS DA DISTORÇÃO A DIREITA IDENTIFICADO NO "data[num_cols].describe()"
data_hist_log = data.copy()

data_hist_log = data_hist_log[data_hist_log.NA_Sales != 0]
data_hist_log = data_hist_log[data_hist_log.EU_Sales != 0]
data_hist_log = data_hist_log[data_hist_log.Other_Sales != 0]
data_hist_log = data_hist_log[data_hist_log.JP_Sales != 0]
data_hist_log = data_hist_log[data_hist_log.Global_Sales != 0]

plt.figure(figsize=(20,10))
sales_cols_log = ['NA_Sales', 'EU_Sales', 'JP_Sales', 'Other_Sales', 'Global_Sales']
for i, column in enumerate(sales_cols_log):
    plt.subplot(3,2,i+1)
    sns.distplot(np.log(data_hist_log[column]), bins=20)

skew_num= pd.DataFrame({'columns':sales_cols_log})
skew_num['skewness']= skew_num['columns'].apply(lambda x:data_hist_log[x].skew())
skew_num

#++++++++++++++ ANÁLISE EXPLORATÓRIA DE DADOS ++++++++++++++

#FREQUÊNCIA DOS GÊNEROS DE JOGOS
plt.figure(figsize=(10, 6))
top10_Genre = data['Genre'].value_counts().nlargest(10) #Define exebir só os 10 primeiros
sns.barplot(x=top10_Genre.values, y=top10_Genre.index, hue=top10_Genre.index, dodge=False, palette='viridis')
plt.title('Frequência dos Gêneros de Jogos')
plt.xlabel('Número de Jogos')
plt.ylabel('Gênero')
plt.show()

#FREQUÊNCIA DAS PUBLISHERS
plt.figure(figsize=(10, 6))
top10_publishers = data['Publisher'].value_counts().nlargest(10)
sns.barplot(x=top10_publishers.values, y=top10_publishers.index, hue=top10_publishers.index, dodge=False, palette='viridis')
plt.title('Top 10 Publishers Mais Frequentes')
plt.xlabel('Número de Jogos')
plt.ylabel('Publisher')
plt.show()

#FREQUÊNCIA DAS PLATAFORMAS
plt.figure(figsize=(10, 6))
top10_Platform = data['Platform'].value_counts().nlargest(10)
sns.barplot(x=top10_Platform.values, y=top10_Platform.index, hue=top10_Platform.index, dodge=False, palette='viridis')
plt.title('Top 10 Plataformas Mais Frequentes')
plt.xlabel('Número de Jogos')
plt.ylabel('Plataforma')
plt.show()

#VENDAS DE GENERO POR REGIÃO

genre_sales = data.groupby('Genre')[['NA_Sales', 'EU_Sales', 'JP_Sales', 'Other_Sales']].sum().reset_index()
genre_sales_melted = genre_sales.melt(id_vars='Genre', var_name='Region', value_name='Sales')


plt.figure(figsize=(12, 8))
sns.barplot(data=genre_sales_melted, x='Genre', y='Sales', hue='Region', palette='muted')
plt.title('Vendas por Gênero e Região')
plt.xlabel('Gênero')
plt.ylabel('Vendas')
plt.xticks(rotation=45, ha='right')
plt.legend(title='Região')
plt.show()

#TOP PUBLISHER POR REGIÃO
top_publishers_na = data.groupby('Publisher')['NA_Sales'].sum().nlargest(10).reset_index()
top_publishers_eu = data.groupby('Publisher')['EU_Sales'].sum().nlargest(10).reset_index()
top_publishers_jp = data.groupby('Publisher')['JP_Sales'].sum().nlargest(10).reset_index()
top_publishers_other = data.groupby('Publisher')['Other_Sales'].sum().nlargest(10).reset_index()


top_publishers = pd.concat([top_publishers_na, top_publishers_eu, top_publishers_jp, top_publishers_other])
top_publishers_melted = top_publishers.melt(id_vars='Publisher', var_name='Region', value_name='Sales')


plt.figure(figsize=(12, 8))
sns.barplot(data=top_publishers_melted, x='Publisher', y='Sales', hue='Region', palette='muted')
plt.title('Top Publishers por Região e Vendas')
plt.xlabel('Publisher')
plt.ylabel('Vendas')
plt.xticks(rotation=45, ha='right')
plt.legend(title='Região')
plt.show()

#VENDAS PLATAFORMA POR REGIÃO
platform_sales = data.groupby('Platform')[['NA_Sales', 'EU_Sales', 'JP_Sales', 'Other_Sales']].sum().reset_index()
platform_sales_melted = platform_sales.melt(id_vars='Platform', var_name='Region', value_name='Sales')

plt.figure(figsize=(12, 8))
sns.barplot(data=platform_sales_melted, x='Platform', y='Sales', hue='Region', palette='muted')
plt.title('Vendas Plataforma e Região')
plt.xlabel('Plataforma')
plt.ylabel('Vendas')
plt.xticks(rotation=45, ha='right')
plt.legend(title='Região')
plt.show()

#TOP GENERO POR ANO
data = data.dropna(subset=['Year'])
data['Year'] = data['Year'].astype(int)
genre_sales_by_year = data.groupby(['Year', 'Genre'])['Global_Sales'].sum().reset_index()
top_genre_by_year = genre_sales_by_year.loc[genre_sales_by_year.groupby('Year')['Global_Sales'].idxmax()]

plt.figure(figsize=(12, 8))
sns.barplot(data=top_genre_by_year, x='Year', y='Global_Sales', hue='Genre', palette='muted')
plt.title('Top Gênero por Ano em Vendas Globais')
plt.xlabel('Ano')
plt.ylabel('Vendas Globais')
plt.xticks(rotation=45)
plt.legend(title='Gênero')
plt.show()

# #TOP PLATAFORMA POR ANO
platform_sales_by_year = data.groupby(['Year', 'Platform'])['Global_Sales'].sum().reset_index()
top_platforms_by_year = platform_sales_by_year.loc[platform_sales_by_year.groupby('Year')['Global_Sales'].idxmax()]

plt.figure(figsize=(12, 8))
sns.barplot(data=top_platforms_by_year, x='Year', y='Global_Sales', hue='Platform', palette='muted')
plt.title('Top Plataforma por Ano em Vendas Globais')
plt.xlabel('Ano')
plt.ylabel('Vendas Globais')
plt.xticks(rotation=45)
plt.legend(title='Plataforma')
plt.show()

#TOP PUBLISHER POR ANO
publisher_sales_by_year = data.groupby(['Year', 'Publisher'])['Global_Sales'].sum().reset_index()
top_publishers_by_year = publisher_sales_by_year.loc[publisher_sales_by_year.groupby('Year')['Global_Sales'].idxmax()]

plt.figure(figsize=(12, 8))
sns.barplot(data=top_publishers_by_year, x='Year', y='Global_Sales', hue='Publisher', palette='muted')
plt.title('Top Publisher por Ano em Vendas Globais')
plt.xlabel('Ano')
plt.ylabel('Vendas Globais')
plt.xticks(rotation=45)
plt.legend(title='Publisher')
plt.show()

#TOP JOGOS POR GENERO
top_games_index_by_genre = data.groupby('Genre')['Global_Sales'].idxmax()
top_games_by_genre = data.loc[top_games_index_by_genre]

plt.figure(figsize=(12, 8))
sns.barplot(data=top_games_by_genre, x='Genre', y='Global_Sales', hue='Name', palette='muted')
plt.title('Top Jogo em Vendas por Gênero')
plt.xlabel('Gênero')
plt.ylabel('Vendas Globais')
plt.xticks(rotation=45)
plt.legend(title='Jogo')
plt.show()

#TOP JOGOS GENERO AÇÃO
action_games = data[data['Genre'] == 'Action']
top_action_games = action_games.sort_values(by='Global_Sales', ascending=False)

plt.figure(figsize=(12, 8))
sns.barplot(data=top_action_games.head(30), x='Name', y='Global_Sales', palette='viridis')
plt.title('Top Jogos em Vendas do Gênero Ação')
plt.xlabel('Nome do Jogo')
plt.ylabel('Vendas Globais')
plt.xticks(rotation=45, ha='right')
plt.show()

#TOP JOGOS GENERO SPORTS
action_games = data[data['Genre'] == 'Sports']
top_action_games = action_games.sort_values(by='Global_Sales', ascending=False)

plt.figure(figsize=(12, 8))
sns.barplot(data=top_action_games.head(30), x='Name', y='Global_Sales', palette='viridis')
plt.title('Top Jogos em Vendas do Gênero Sports')
plt.xlabel('Nome do Jogo')
plt.ylabel('Vendas Globais')
plt.xticks(rotation=45, ha='right')
plt.show()

#contagem qtd de jogos por genero
genre_counts = data['Genre'].value_counts()
print(genre_counts)

#TOP PUBLISHER POR GENERO
top_games_index_by_publisher = data.groupby('Publisher')['Global_Sales'].idxmax()
top_publishers = data.groupby('Publisher')['Global_Sales'].sum().nlargest(15).index
top_games_by_top_publishers = data[data['Publisher'].isin(top_publishers)]
top_games_index_by_publisher = top_games_by_top_publishers.groupby('Publisher')['Global_Sales'].idxmax()
top_games_by_publisher = top_games_by_top_publishers.loc[top_games_index_by_publisher]


plt.figure(figsize=(12, 8))
sns.barplot(data=top_games_by_publisher, x='Publisher', y='Global_Sales', hue='Name', palette='muted')
plt.title('Top Jogo em Vendas para os Top 10 Publishers')
plt.xlabel('Publisher')
plt.ylabel('Vendas Globais')
plt.xticks(rotation=45)
plt.legend(title='Jogo')
plt.show()

#TOP VENDAS POR PLATAFORMA
top_games_index_by_platform = data.groupby('Platform')['Global_Sales'].idxmax()
top_games_by_platform = data.loc[top_games_index_by_platform]


plt.figure(figsize=(12, 8))
sns.barplot(data=top_games_by_platform, x='Platform', y='Global_Sales', hue='Name', palette='muted')
plt.title('Top Jogo em Vendas por Plataforma')
plt.xlabel('Plataforma')
plt.ylabel('Vendas Globais')
plt.xticks(rotation=45)
plt.legend(title='Jogo', bbox_to_anchor=(1.05, 1), loc='upper left')
plt.show()

#MATRIZ DE CORRELAÇÃO DE DADOS
fig,ax=plt.subplots(figsize=(9,7))
sns.heatmap(data[num_cols].corr().round(4), annot=True)
ax.set_title('MATRIZ DE CORRELAÇÃO VARIÁVEIS NÚMERICAS', fontsize=15);

#PAIRPLOT CAMPOS DE VENDAS E ANO
sns.pairplot(data, vars=['Year', 'NA_Sales', 'EU_Sales', 'JP_Sales', 'Other_Sales', 'Global_Sales'], hue='Genre')
plt.show()

#SCCATER GLOBAL_SALES POR GENERO AO LONGO DOS ANOS
plt.figure(figsize=(10, 6))
sns.scatterplot(data=data, x='Year', y='Global_Sales', hue='Genre', palette='muted')
plt.title('Vendas por Gênero ao Longo dos Anos')
plt.xlabel('Ano')
plt.ylabel('Vendas Globais')
plt.xticks(rotation=45)
plt.legend(title='Gênero')
plt.show()

#SCCATER NA_SALES POR GENERO AO LONGO DOS ANOS
plt.figure(figsize=(10, 6))
sns.scatterplot(data=data, x='Year', y='NA_Sales', hue='Genre', palette='muted')
plt.title('Vendas na América do Norte por Gênero ao Longo dos Anos')
plt.xlabel('Ano')
plt.ylabel('Vendas na América do Norte')
plt.xticks(rotation=45)
plt.legend(title='Gênero')
plt.show()

#SCCATER VENDAS JP_SALES POR GENERO AO LONGO DOS ANOS
plt.figure(figsize=(10, 6))
sns.scatterplot(data=data, x='Year', y='JP_Sales', hue='Genre', palette='muted')
plt.title('Vendas no Japão e por Gênero ao Longo dos Anos')
plt.xlabel('Ano')
plt.ylabel('Vendas no Japão')
plt.xticks(rotation=45)
plt.legend(title='Gênero')
plt.show()

#REGPLOT ENTRE GLOBAL_SALES POR NA_SALES (MAIOR CORRELAÇÃO 0.93) -  POSSIVEL INDENTIFICAÇÃO DE OUTLINER 'WII SPORTS'
g = sns.regplot(x=data.Global_Sales, y=data.NA_Sales, ci=None, scatter_kws={"color": "r", "s": 9})
plt.xlim(-2, 85)
plt.ylim(bottom=0)

data.head()

data = data.drop([0],axis=0) #DROP WII SPORTS

g = sns.regplot(x=data.Global_Sales, y=data.NA_Sales, ci=None, scatter_kws={"color": "r", "s": 9})
plt.xlim(-2, 85)
plt.ylim(bottom=0)

data.head()

#--------------MODELAGEM - DECISION TREE--------------

from sklearn.preprocessing import LabelEncoder

dff = data.copy()

le = LabelEncoder() #converter valores categóricos em números

feature = ["Platform", "Genre"]


for col in feature:
    dff[col] = le.fit_transform(data[col]) #Platform e Genre são transformadas em valores numéricos.

dff.head()

X = dff[['Platform', 'Genre', 'NA_Sales', 'EU_Sales', 'JP_Sales', 'Other_Sales']].values #features

y = dff['Global_Sales'].values #target

X.shape

y.shape

from sklearn.model_selection import train_test_split
x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42) #25% dos dados serão usados para teste e 75% para treinamento
#"random_state=42" garante que a divisão seja a mesma toda vez que o código é executado.

from sklearn.preprocessing import RobustScaler
ro = RobustScaler() #normalizar os dados, reduzindo o impacto de outliers

x_train = ro.fit_transform(x_train) #ajusta o escalador aos dados de treinamento e os transforma
x_test = ro.fit_transform(x_test) #transforma os dados de teste de acordo com a mesma escala.

from sklearn.metrics import accuracy_score, confusion_matrix, f1_score,classification_report, roc_curve

regressor = DecisionTreeRegressor() #instancia para criar um modelo de regressão DecisionTree
regressor.fit(x_train, y_train) #treina o modelo usando os dados de treinamento x_train e y_train

regressor.score(x_train,y_train) #calcula o coeficiente de determinação (R^2) modelo de treinamento

regressor.score(x_test,y_test) #calcula o coeficiente de determinação (R^2) modelo de teste

yTreePre= regressor.predict(x_test)

r2Tree= r2_score(y_test ,yTreePre ) #calcula o R^2 das previsões
r2Tree

from sklearn.metrics import mean_absolute_error
maeTREE = mean_absolute_error(yTreePre, y_test) #calcula o erro absoluto médio das previsões
print('Mean absolute error '+str(maeTREE))

db = pd.DataFrame({'Atual': y_test, 'Previsto': yTreePre})
db

test = pd.DataFrame({'Previsto':yTreePre,'Atual':y_test})
fig= plt.figure(figsize=(16,8))
test = test.reset_index()
test = test.drop(['index'],axis=1)
sns.jointplot(x='Atual',y='Previsto',data=test,kind='reg',);

db1 = db.head()
db1.plot(kind='line',figsize=(15,8))
plt.show()

regressor.feature_importances_ #retorna a importância de cada feature. 'Platform', 'Genre', 'NA_Sales', 'EU_Sales', 'JP_Sales', 'Other_Sales'

#--------------MODELAGEM - RANDOM FOREST REGRESSOR--------------

rf = RandomForestRegressor() # instancia para criar um modelo de regressão random forest
rf.fit(x_train , y_train) #treina o modelo usando os dados de treinamento x_train e y_train

rf.score(x_train,y_train) #calcula o coeficiente de determinação (R^2) modelo de treinamento

rf.score(x_test,y_test) #calcula o coeficiente de determinação (R^2) modelo de teste

yRandomPre= rf.predict(x_test) #fazer previsões no conjunto de teste

r2Random = r2_score(y_test , yRandomPre) #calculo R^2 as previsões comparando os valores reais
r2Random

from sklearn.metrics import mean_absolute_error
maeRandom = mean_absolute_error(yRandomPre, y_test)  #calcula o erro absoluto médio das previsões
print('Mean absolute error '+str(maeRandom))

dbRandom = pd.DataFrame({'Atual': y_test, 'Previsto': yRandomPre})
dbRandom

test = pd.DataFrame({'Previsto':yRandomPre,'Atual':y_test})
fig= plt.figure(figsize=(16,8))
test = test.reset_index()
test = test.drop(['index'],axis=1)
sns.jointplot(x='Atual',y='Previsto',data=test,kind='reg',);

dbRandom1 = dbRandom.head()
dbRandom1.plot(kind='line',figsize=(15,8))
plt.show()

rf.feature_importances_ #retorna a importância de cada feature. 'Platform', 'Genre', 'NA_Sales', 'EU_Sales', 'JP_Sales', 'Other_Sales'

#--------------MODELAGEM - KNN REGRESSOR --------------

knn=KNeighborsRegressor() # instancia para criar um modelo de regressão KNN
knn.fit(x_train, y_train) #treina o modelo usando os dados de treinamento x_train e y_train

knn.score(x_train,y_train) #calcula o coeficiente de determinação (R^2) modelo de treinamento

knn.score(x_test , y_test) #calcula o coeficiente de determinação (R^2) modelo de teste

pred = knn.predict(x_test) #fazer previsões no conjunto de teste

r2_knn = r2_score(y_test,pred) #calculo R^2 as previsões comparando os valores reais
print(r2_knn)

from sklearn.metrics import mean_absolute_error
maeKNN = mean_absolute_error(pred, y_test) #calcula o erro absoluto médio das previsões
print('Mean absolute error '+str(maeKNN))

dbKNN = pd.DataFrame({'Atual': y_test, 'Previsto': pred})
dbKNN

test = pd.DataFrame({'Previsto':pred,'Atual':y_test})
fig= plt.figure(figsize=(16,8))
test = test.reset_index()
test = test.drop(['index'],axis=1)
sns.jointplot(x='Atual',y='Previsto',data=test,kind='reg',);

dbKNN1 = dbKNN.head()
dbKNN1.plot(kind='line',figsize=(15,8))
plt.show()

#--------------MODELAGEM - REGRESSOR LINEAR --------------

lr = LinearRegression() #instancia para criar um modelo de regressão linear
lr.fit(x_train,y_train) #treina o modelo usando os dados de treinamento x_train e y_train

lr.score(x_train,y_train) #calcula o coeficiente de determinação (R^2) modelo de treinamento

lr.score(x_test,y_test) #calcula o coeficiente de determinação (R^2) modelo de teste

yLinearPre = lr.predict(x_test) #fazer previsões no conjunto de teste

r2Linear = r2_score(y_test , yLinearPre) #calculo R^2 as previsões comparando os valores reais
r2Linear

from sklearn.metrics import mean_absolute_error
maeRL = mean_absolute_error(yLinearPre, y_test) #calcula o erro absoluto médio das previsões
print('Mean absolute error '+str(maeRL))

dbLinear = pd.DataFrame({'Atual': y_test, 'Previsto': yLinearPre})
dbLinear

test = pd.DataFrame({'Previsto':yLinearPre,'Atual':y_test})
fig= plt.figure(figsize=(16,8))
test = test.reset_index()
test = test.drop(['index'],axis=1)
sns.jointplot(x='Atual',y='Previsto',data=test,kind='reg',);

dbLinear1 = dbLinear.head()
dbLinear1.plot(kind='line',figsize=(15,8))
plt.show()

lr.coef_ #retorna a importância de cada feature. 'Platform', 'Genre', 'NA_Sales', 'EU_Sales', 'JP_Sales', 'Other_Sales'

#COMPARAÇÃO MODELOS ORDENADOS PELO R2 SCORE
metrics_df = pd.DataFrame({
    'Modelo': ['Decision Tree', 'Random Forest', 'K-Nearest Neighbors', 'Linear Regression'],
    'R2 Score': [r2Tree, r2Random, r2_knn, r2Linear],
    'MAE': [maeTREE, maeRandom, maeKNN, maeRL]
})

metrics_df_sorted = metrics_df.sort_values(by='R2 Score', ascending=False)
metrics_df_sorted

#COMPARAÇÃO MODELOS ORDENADOS PELO ERRO MEDIO ABSOLUTO
metrics_df = pd.DataFrame({
    'Modelo': ['Decision Tree', 'Random Forest', 'K-Nearest Neighbors', 'Linear Regression'],
    'R2 Score': [r2Tree, r2Random, r2_knn, r2Linear],
    'MAE': [maeTREE, maeRandom, maeKNN, maeRL]
})

metrics_df_sorted = metrics_df.sort_values(by='MAE')
metrics_df_sorted